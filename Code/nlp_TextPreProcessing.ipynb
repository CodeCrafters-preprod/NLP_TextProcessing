{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tweetID       entity sentiment  \\\n",
      "0     2401  Borderlands  Positive   \n",
      "1     2401  Borderlands  Positive   \n",
      "2     2401  Borderlands  Positive   \n",
      "3     2401  Borderlands  Positive   \n",
      "4     2401  Borderlands  Positive   \n",
      "\n",
      "                                       tweet_content  \\\n",
      "0  im getting on borderlands and i will murder yo...   \n",
      "1  I am coming to the borders and I will kill you...   \n",
      "2  im getting on borderlands and i will kill you ...   \n",
      "3  im coming on borderlands and i will murder you...   \n",
      "4  im getting on borderlands 2 and i will murder ...   \n",
      "\n",
      "                 processed_content  \\\n",
      "0       im get borderland murder ,   \n",
      "1            come border kill all,   \n",
      "2      im get borderland kill all,   \n",
      "3   im come borderland murder all,   \n",
      "4  im get borderland 2 murder all,   \n",
      "\n",
      "                                     word2vec_vector  \\\n",
      "0  [-1.0745261, 0.5151903, -0.5766882, -0.7825491...   \n",
      "1  [-0.5513947, 0.3456574, -0.13432187, -0.499555...   \n",
      "2  [-0.9943293, 0.40885267, -0.48143703, -0.75621...   \n",
      "3  [-0.9947753, 0.4470769, -0.51506764, -0.870741...   \n",
      "4  [-1.1171185, 0.5231082, -0.52621335, -0.539582...   \n",
      "\n",
      "                                        glove_vector  \n",
      "0  [0.22772, 0.0013851999999999949, 0.3162134, -0...  \n",
      "1  [-0.28461833333333336, -0.16813666666666668, 0...  \n",
      "2  [0.089055, 0.04352899999999999, 0.37232425, -0...  \n",
      "3  [0.25339875, 0.014191499999999996, 0.24798925,...  \n",
      "4  [0.171896, 0.10174520000000001, 0.226181400000...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "# Load Twitter data\n",
    "twitter_data = pd.read_csv('../Data/twitter_data.csv')\n",
    "twitter_data.dropna(subset=['tweet_content'], inplace=True)  # Drop rows where tweet_content is missing\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Tokenization and removing stopwords\n",
    "    tokens = [word for word in text.lower().split() if word not in stop_words]\n",
    "    # Stemming\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    # Lemmatization\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in stemmed]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Apply preprocessing\n",
    "twitter_data['processed_content'] = twitter_data['tweet_content'].apply(preprocess_text)\n",
    "\n",
    "# Count Vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(twitter_data['processed_content'])\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(twitter_data['processed_content'])\n",
    "\n",
    "# Word2Vec embeddings\n",
    "sentences = [tweet.split() for tweet in twitter_data['processed_content']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get sentence vector by averaging Word2Vec embeddings\n",
    "def get_sentence_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# Apply Word2Vec\n",
    "twitter_data['word2vec_vector'] = twitter_data['processed_content'].apply(lambda x: get_sentence_vector(x, word2vec_model))\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_model(glove_file):\n",
    "    glove_model = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array([float(val) for val in split_line[1:]])\n",
    "            glove_model[word] = embedding\n",
    "    return glove_model\n",
    "\n",
    "# Specify the path to your downloaded GloVe embeddings\n",
    "glove_file_path = 'C:/Users/HP/Downloads/glove.6B/glove.6B.100d.txt'  \n",
    "glove_model = load_glove_model(glove_file_path)\n",
    "\n",
    "# Function to get GloVe vector for a sentence\n",
    "def get_glove_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(100)  # 100d embeddings\n",
    "\n",
    "# Apply GloVe embeddings\n",
    "twitter_data['glove_vector'] = twitter_data['processed_content'].apply(lambda x: get_glove_vector(x, glove_model))\n",
    "\n",
    "# Display processed data\n",
    "print(twitter_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7827333333333333\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.83      0.68      0.75      2666\n",
      "    Negative       0.78      0.84      0.81      4464\n",
      "     Neutral       0.81      0.74      0.77      3706\n",
      "    Positive       0.74      0.83      0.78      4164\n",
      "\n",
      "    accuracy                           0.78     15000\n",
      "   macro avg       0.79      0.77      0.78     15000\n",
      "weighted avg       0.79      0.78      0.78     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(twitter_data['processed_content'])\n",
    "y = twitter_data['sentiment']  # Assuming 'sentiment' is the target variable\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)  # Increase max_iter if you encounter convergence warnings\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
